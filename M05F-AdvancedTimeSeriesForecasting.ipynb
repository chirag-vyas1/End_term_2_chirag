{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563cb68c",
   "metadata": {
    "id": "563cb68c"
   },
   "source": [
    "![Cloud-First](../image/CloudFirst.png)\n",
    "\n",
    "\n",
    "# SIT742: Modern Data Science\n",
    "**(Module: Big Data Manipulation)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
    "\n",
    "\n",
    "Prepared by **SIT742 Teaching Team**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Session 5F: Advanced Time Series Forecasting\n",
    "---\n",
    "\n",
    "### Table of Content\n",
    "\n",
    "Part A: Data Exploration and Temporal Feature Set Up\n",
    "\n",
    "1. Download and Preprocess Data\n",
    "\n",
    "2. Prepare Dataset for LSTM\n",
    "\n",
    "3. Define LSTM with Attention\n",
    "4. Train the Model\n",
    "5. Make Predictions (Ex-post) and Evaluation\n",
    "\n",
    "Part B: Attension Score Exploration\n",
    "\n",
    "6. Construct the Attention Mechanism\n",
    "\n",
    "7. Plot the Averaged Attention Score\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction ##\n",
    "\n",
    "This notebook demonstrates how to build a advanced time series forecasting model for stock prices using an LSTM with an attention mechanism. It walks through downloading and preprocessing historical Apple stock data, preparing the dataset for the model, defining and training the LSTM-Attention network, and evaluating the predictions. Additionally, it visualizes the modelâ€™s attention scores to interpret which input features (Open, High, Low prices) the model focuses on when making forecasts. This example provides a practical introduction to combining deep learning and attention mechanisms for time series prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8503687",
   "metadata": {
    "id": "f8503687"
   },
   "source": [
    "# Part A: Data Exploration and Temporal Feature Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd4a06",
   "metadata": {
    "id": "78bd4a06"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c15acc",
   "metadata": {
    "id": "c6c15acc"
   },
   "source": [
    "## 1: Download and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d092242",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d092242",
    "outputId": "cd4a2e16-1fb8-4a7b-db34-17c01cd84c38"
   },
   "outputs": [],
   "source": [
    "aapl_data = yf.download('AAPL', start='2020-01-01', end='2023-01-01')\n",
    "close_prices = aapl_data[['Open','High','Low']].values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "close_prices_scaled = scaler.fit_transform(close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9e7a8",
   "metadata": {
    "id": "bbd9e7a8"
   },
   "source": [
    "## 2: Prepare Dataset for LSTM\n",
    "Data Preparation:\n",
    "**Input (X):**\n",
    "\n",
    "`X = close_prices_scaled[:-1]:`\n",
    "Uses all but the last row of normalized stock data (Open, High, Low).\n",
    "\n",
    "**Target (y):**\n",
    "\n",
    "`y = close_prices_scaled[1:, 2]:`\n",
    "Uses the Low price from the next day (column index 2) as the target.\n",
    "\n",
    "**Reshape for LSTM:**\n",
    "\n",
    "`X = X.reshape(X.shape[0], 3, 1):`\n",
    "\n",
    "**Reshapes X into 3D format:**\n",
    "(samples, sequence_length=3, features=1)\n",
    "\n",
    "Each input sequence contains 3 time steps (Open, High, Low).\n",
    "\n",
    "`y = y.reshape(-1, 1):`\n",
    "\n",
    "Flattens the target into a column vector of shape (samples, 1).\n",
    "\n",
    "**Train-Test Split:**\n",
    "\n",
    "`train_test_split(..., test_size=0.2, random_state=42):`\n",
    "\n",
    "Splits the dataset into 80% training and 20% testing.\n",
    "\n",
    "**Convert to PyTorch Tensors:**\n",
    "\n",
    "`torch.tensor(..., dtype=torch.float32):`\n",
    "\n",
    "Converts all datasets (X_train, X_test, y_train, y_test) to PyTorch tensors required for model training. *italicised text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d4b4b",
   "metadata": {
    "id": "144d4b4b"
   },
   "outputs": [],
   "source": [
    "X = close_prices_scaled[:-1]\n",
    "y = close_prices_scaled[1:, 2]\n",
    "X = X.reshape(X.shape[0], 3, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da0f9e",
   "metadata": {
    "id": "60da0f9e"
   },
   "source": [
    "## 3: Define LSTM with Attention\n",
    "\n",
    "*This module defines an LSTM model enhanced with an attention mechanism for sequence modeling. It initializes an LSTM layer that processes input sequences and outputs hidden states. The attention mechanism learns to assign importance weights to each timestep by passing the LSTM outputs through a linear layer followed by a softmax, creating a probability distribution over timesteps. These attention weights are then used to compute a weighted sum of the LSTM outputs, producing a context vector that highlights the most relevant information from the sequence. Finally, this context vector is fed through a fully connected layer to generate the output prediction. The forward method optionally returns the attention weights alongside the output for interpretability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408010bf",
   "metadata": {
    "id": "408010bf"
   },
   "outputs": [],
   "source": [
    "class LSTMAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1):\n",
    "        super(LSTMAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out).squeeze(-1), dim=-1)\n",
    "        context_vector = torch.sum(lstm_out * attention_weights.unsqueeze(-1), dim=1)\n",
    "        out = self.fc(context_vector)\n",
    "        if return_attention:\n",
    "            return out, attention_weights\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f322c4",
   "metadata": {
    "id": "02f322c4"
   },
   "source": [
    "## 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919f429",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5919f429",
    "outputId": "2e201364-2633-4da6-8043-b1bb2641f82f"
   },
   "outputs": [],
   "source": [
    "model = LSTMAttention(input_dim=1, hidden_dim=50)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        test_pred = model(X_test_tensor)\n",
    "        test_loss = criterion(test_pred, y_test_tensor)\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26bfe2c",
   "metadata": {
    "id": "b26bfe2c"
   },
   "source": [
    "## 5: Make Predictions (Ex-post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e92b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "ab7e92b8",
    "outputId": "9a2a1d21-be00-4d0e-a5cd-57585c208637"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = model(X_test_tensor).detach().numpy()\n",
    "predictions_actual = scaler.inverse_transform(np.hstack((np.zeros_like(predictions), np.zeros_like(predictions), predictions)))[:, 2:]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(scaler.inverse_transform(np.hstack((np.zeros_like(y_test), np.zeros_like(y_test), y_test)))[:, 2:], label='Actual')\n",
    "plt.plot(predictions_actual, label='Predicted')\n",
    "plt.title('AAPL Stock Price Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6095f",
   "metadata": {
    "id": "aab6095f"
   },
   "source": [
    "## 6: Calculate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce254f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55ce254f",
    "outputId": "2cc23a10-98fe-4d2d-e1c4-37015dbb0e1d"
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(scaler.inverse_transform(np.hstack((np.zeros_like(y_test), np.zeros_like(y_test), y_test)))[:, 2:], predictions_actual)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LQ8rQZzlqPnn",
   "metadata": {
    "id": "LQ8rQZzlqPnn"
   },
   "source": [
    "# Part B: Attension Score Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OwAnmO9tsLMU",
   "metadata": {
    "id": "OwAnmO9tsLMU"
   },
   "source": [
    "## 1: Construct with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEz-OEDLqhTg",
   "metadata": {
    "id": "lEz-OEDLqhTg"
   },
   "outputs": [],
   "source": [
    "all_attention_weights = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X_test_tensor)):\n",
    "        x = X_test_tensor[i].unsqueeze(0)  # shape [1, seq_len, 1]\n",
    "        _, attn = model(x, return_attention=True)\n",
    "        all_attention_weights.append(attn.squeeze(0).numpy())  # shape [seq_len]\n",
    "\n",
    "# Convert to numpy array: shape [num_samples, seq_len]\n",
    "all_attention_weights = np.stack(all_attention_weights)\n",
    "\n",
    "# Compute average across all samples (mean across axis=0)\n",
    "average_attention = np.mean(all_attention_weights, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WvRDEDgIsUvr",
   "metadata": {
    "id": "WvRDEDgIsUvr"
   },
   "source": [
    "## 2: Plot the Averaged Attention Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wz6YYgwMqnEf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Wz6YYgwMqnEf",
    "outputId": "89d6f0f9-0e62-43b2-d4f3-f05e2baf081a"
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(average_attention, marker='o')\n",
    "plt.title(\"Average Attention Score Across All Test Samples\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Average Attention Weight\")\n",
    "plt.xticks(ticks=[0, 1, 2], labels=['Open', 'High', 'Low'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uGSS4C6OrmUp",
   "metadata": {
    "id": "uGSS4C6OrmUp"
   },
   "source": [
    "# Now your turn, could you please have a look of the given model structure and find out how it works with attension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bxZjm1r2Jp",
   "metadata": {
    "id": "82bxZjm1r2Jp"
   },
   "outputs": [],
   "source": [
    "# import configuration\n",
    "# from keras.utils import np_utils\n",
    "# from keras.layers import *\n",
    "# from keras.models import *\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.regularizers import l2,l1\n",
    "# from keras.initializers import *\n",
    "\n",
    "\n",
    "# def attention_3d_block_f(inputs):\n",
    "#     # a = Permute((2, 1))(inputs)\n",
    "#     a = Dense(input_dim+1, activation='relu', use_bias=True)(inputs)\n",
    "#     a = Activation('softmax')(a)\n",
    "#     # a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "#     output_attention_mul = multiply([inputs, a], name='attention_mul')\n",
    "#     return output_attention_mul\n",
    "\n",
    "\n",
    "# def attention_3d_block_m(inputs):\n",
    "#     a = Permute((2, 1))(inputs)\n",
    "#     a = Dense(12, activation='relu', use_bias=True)(a)\n",
    "#     a = Activation('softmax')(a)\n",
    "#     a_probs = Permute((2, 1), name='attention_vec1')(a)\n",
    "#     output_attention_mul = multiply([inputs, a_probs], name='attention_mul1')\n",
    "#     return output_attention_mul\n",
    "# # build RNN model with attention\n",
    "\n",
    "# def buildmodel(train_x,train_y,test_x,test_y,dim1,dim2,dim3,dropout_1,batchsize,epoch):\n",
    "#     inputs2 = Input(shape=(12, dim1))\n",
    "#     attention_mul1 = attention_3d_block_f(inputs2)\n",
    "#     # attention_mul2 = attention_3d_block_m(inputs2)\n",
    "#     lstm_out2 = LSTM(dim2, return_sequences=False, activation='relu', dropout=0.01, kernel_initializer=he_normal(),\n",
    "#                      name='bilstm1')(\n",
    "#         attention_mul1)\n",
    "\n",
    "#     drop2 = Dropout(dropout_1)(lstm_out2)\n",
    "#     output1 = Dense(dim3, kernel_initializer=he_normal(), activation='relu')(drop2)\n",
    "#     output1 = Dense(1, kernel_initializer=he_normal())(output1)\n",
    "#     model2 = Model(inputs=inputs2, outputs=output1)\n",
    "#     model2.compile(loss='mape', optimizer='adam')\n",
    "#     #model2.summary()\n",
    "#     model2.fit(train_x,train_y, batch_size=batchsize, epochs=epoch, callbacks=cbks, verbose=1,\n",
    "#                validation_data=(test_x,test_y))\n",
    "#     return model2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
